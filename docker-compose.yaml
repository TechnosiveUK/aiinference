# PrivaXAI AI Inference Stack - Docker Compose Configuration
# Purpose: Production-grade AI inference stack with GPU support for PrivaXAI SaaS platform
# Hardware: NVIDIA T4 (16GB VRAM), 32GB RAM, 8 cores, Ubuntu 24.04 LTS

services:
  # Ollama - GPU-accelerated LLM inference engine
  # Purpose: Runs Qwen 2.5 Coder 7B model on NVIDIA T4 GPU
  # Not exposed publicly - only accessible via TensorZero gateway
  ollama:
    image: ollama/ollama:latest
    container_name: privaxai-ollama
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    volumes:
      # Model storage (persists across restarts)
      - ./ollama:/root/.ollama
      # Logs for debugging
      - ./logs/ollama:/var/log/ollama
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
      - OLLAMA_KEEP_ALIVE=5m
      # Limit GPU memory usage to prevent OOM
      - CUDA_VISIBLE_DEVICES=0
    networks:
      - ai-stack-internal
    healthcheck:
      test: ["CMD-SHELL", "wget --spider --quiet http://localhost:11434/api/tags || curl -f http://localhost:11434/api/tags || exit 0"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s  # Ollama can take time to start, especially on first run
    # Internal port only - not exposed to host
    expose:
      - "11434"

  # Usage Service - Token limit enforcement and usage dashboard API
  # Purpose: Enforces monthly token limits per tier and provides usage statistics
  # Used by TensorZero gateway to check limits before processing requests
  usage-service:
    build:
      context: ./services
      dockerfile: Dockerfile
    container_name: privaxai-usage-service
    restart: unless-stopped
    depends_on:
      clickhouse:
        condition: service_healthy
    environment:
      - CLICKHOUSE_HOST=clickhouse
      - CLICKHOUSE_PORT=9000
      - CLICKHOUSE_DB=tensorzero
    networks:
      - ai-stack-internal
    # Internal port only - not exposed to host
    expose:
      - "8080"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # TensorZero Gateway - LLM Gateway for routing and observability
  # Purpose: Routes requests to Ollama, handles tenant headers, logs to ClickHouse
  # This is the ONLY service that PrivaXAI platform should call
  gateway:
    image: tensorzero/gateway:latest
    container_name: privaxai-gateway
    restart: unless-stopped
    depends_on:
      ollama:
        condition: service_healthy
      clickhouse:
        condition: service_healthy
    volumes:
      # Mount tensorzero.toml configuration
      - ./config:/app/config:ro
    command: --config-file /app/config/tensorzero.toml
    environment:
      - TENSORZERO_CLICKHOUSE_URL=http://clickhouse:8123/tensorzero
      # Optional: Set if you want to use OpenAI API key authentication
      # - OPENAI_API_KEY=${OPENAI_API_KEY:-}
    networks:
      - ai-stack-internal
    ports:
      # Exposed internally for PrivaXAI platform to call
      # In production, this should be behind a reverse proxy or VPN
      - "8000:3000"  # TensorZero gateway uses port 3000 internally
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # ClickHouse - Telemetry and token usage tracking
  # Purpose: Stores inference metrics, token counts, and usage data for billing/metering
  # Not exposed publicly - only accessible via TensorZero
  clickhouse:
    image: clickhouse/clickhouse-server:latest
    container_name: privaxai-clickhouse
    restart: unless-stopped
    volumes:
      # Persistent data storage
      - ./clickhouse_data:/var/lib/clickhouse
      # Configuration (if needed)
      - ./config/clickhouse:/etc/clickhouse-server/config.d:ro
      # Logs
      - ./logs/clickhouse:/var/log/clickhouse
    environment:
      - CLICKHOUSE_DB=tensorzero
      # Disable public HTTP interface (only internal access)
      - CLICKHOUSE_HTTP_PORT=8123
    networks:
      - ai-stack-internal
    # Internal port only - not exposed to host
    expose:
      - "8123"
      - "9000"
    healthcheck:
      test: ["CMD", "clickhouse-client", "--query", "SELECT 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    ulimits:
      nofile:
        soft: 262144
        hard: 262144

# Internal network - isolates AI stack from external access
# PrivaXAI platform connects to this network or calls gateway via private IP
networks:
  ai-stack-internal:
    driver: bridge
    internal: false  # Set to true if you want complete isolation (requires VPN/proxy)

# Named volumes for persistent data (optional, using bind mounts above for simplicity)
volumes:
  ollama_data:
    driver: local
  clickhouse_data:
    driver: local

