# TensorZero LLM Gateway Configuration for PrivaXAI
# Purpose: Routes requests to Ollama, provides observability, and prepares for multi-model routing

[server]
# Gateway listens on internal network only (not exposed publicly)
host = "0.0.0.0"
port = 8000

[logging]
level = "info"
format = "json"

# Model configuration for Qwen 2.5 Coder 7B
# This model is optimized for code generation and chat use cases
[models.qwen2.5-coder-7b]
# Ollama model name (must match what's pulled in Ollama)
name = "qwen2.5-coder:7b"
provider = "ollama"
base_url = "http://ollama:11434"

# Model capabilities
[models.qwen2.5-coder-7b.capabilities]
chat = true
completion = true
streaming = true

# Context limits (conservative for T4 16GB VRAM)
[models.qwen2.5-coder-7b.limits]
max_tokens = 8192
max_context = 16384
temperature = 0.7
top_p = 0.9

# Default model for chat and copilot endpoints
[defaults]
chat_model = "qwen2.5-coder:7b"
completion_model = "qwen2.5-coder:7b"

# Observability and telemetry
[observability]
enabled = true
# ClickHouse for production-grade tenant metering
telemetry_backend = "clickhouse"
clickhouse_url = "http://clickhouse:8123"
clickhouse_database = "tensorzero"
clickhouse_table = "llm_requests"  # Production table
# Legacy table for backward compatibility
clickhouse_legacy_table = "token_usage"

# Rate limiting - Per-tier configuration
[rate_limiting]
enabled = true

# Per-tier rate limits (matches PRICING_TIERS.md)
[rate_limiting.tiers.starter]
rpm = 60          # requests per minute
tpm = 100000      # tokens per minute
max_context = 4096
monthly_token_limit = 5000000  # 5M tokens

[rate_limiting.tiers.pro]
rpm = 120
tpm = 250000
max_context = 8192
monthly_token_limit = 25000000  # 25M tokens

[rate_limiting.tiers.enterprise]
rpm = 300
tpm = 1000000
max_context = 16384
monthly_token_limit = 100000000  # 100M tokens

# Default (fallback for unknown tiers)
default_rpm = 60
default_tpm = 100000
default_monthly_token_limit = 5000000

# Token limit enforcement
[rate_limiting.token_limits]
enabled = true
check_before_request = true  # Check monthly limit before processing
usage_service_url = "http://usage-service:8080/api/v1/usage/check"
soft_limit_warning = 0.80  # Warn at 80% of limit
hard_limit_reject = true   # Reject requests when limit exceeded

# Tenant identification via headers (PrivaXAI API Contract)
# Required headers for production:
# - Authorization: Bearer <privaxai-service-token>
# - X-Tenant-ID: tenant_123 (required)
# - X-User-ID: user_456 (optional)
# - X-Plan-Tier: pro (optional)
# - X-Request-ID: uuid (optional)
[tenant]
header_name = "x-tenant-id"
require_header = true  # Required in production for audit safety
user_header = "x-user-id"
plan_header = "x-plan-tier"
request_id_header = "x-request-id"

# Authentication
[auth]
enabled = false  # Set to true when service token is configured
bearer_token_header = "Authorization"

# Health check endpoint
[health]
enabled = true
path = "/health"

